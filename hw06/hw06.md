# hw06.Rmd


```r
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(ggmap))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(singer))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(robustbase))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(MASS))
options(knitr.table.format = "markdown")
```



```r
tableFormat<-function(table,title=""){
  table %>%      
    kable("html",caption=title, align=c(rep('c', 5))) %>%
    kable_styling(bootstrap_options = 
                    c("striped", "hover",  "responsive"),
                  position="center",font_size=14)
}
```

Lets update the theme of ggplot to make title of all plots centered.

```r
theme_update(plot.title = element_text(hjust = 0.5))
```


###Task 2
###Writing functions

Lets define the functions for linear regression **le_lin_fit()** , quadratic regression **le_quad_fit()** and **le_robust_fit()**.

```r
le_lin_fit <- function(data,offset = 1952){
the_fit <- lm(gdpPercap ~ I(year - offset), data)
  data.frame(t(coef(the_fit))) %>%
  setNames(c("intercept", "slope"))
}

le_quad_fit <- function(data,offset = 1952){
the_fit <- rlm(gdpPercap ~ I(year - offset), data,method="M")
  data.frame(t(coef(the_fit))) %>%
  setNames(c("intercept", "slope"))
}

le_robust_fit <- function(data,offset = 1952){
the_fit <- lmrob(gdpPercap ~ I(year - offset), data)
data.frame(t(coef(the_fit))) %>%
  setNames(c("intercept", "slope"))
}
```

Lets check the function with some data for India.


```r
j_dat<-subset(gapminder,country=="India")
```

Lets get the cofficients of different regression functions using inbuilt functions.

```r
le_lin_fit(j_dat)
```

```
##   intercept    slope
## 1  286.2625 28.03759
```

```r
le_quad_fit(j_dat)
```

```
##   intercept    slope
## 1  343.9128 24.57857
```

Let plot this data.

```r
ggplot(j_dat, aes(x = year, y = lifeExp))+ 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

![](hw06_files/figure-html/unnamed-chunk-7-1.png)<!-- -->

```r
ggplot(j_dat, aes(x = year, y = lifeExp))+ 
  geom_point() + 
  geom_smooth(method = "rlm", se = FALSE)
```

![](hw06_files/figure-html/unnamed-chunk-7-2.png)<!-- -->


For the function le_robust_fit, lets apply it to the entire gapminder and merge it to visualize the data.

```r
gapminder_mod<- gapminder%>%
                  group_by(country,continent)%>%
                  do(le_robust_fit(.))%>%
                  full_join(gapminder,.,by=c("country", "continent"))
```

```
## Warning in lmrob.fit(x, y, control, init = init, mf = mf): M-step did NOT
## converge. Returning unconverged SM-estimate
```

```
## Warning in lmrob.S(x, y, control = control, mf = mf): find_scale() did not
## converge in 'maxit.scale' (= 200) iterations
```

```
## Warning in lmrob.fit(x, y, control, init = init, mf = mf): M-step did NOT
## converge. Returning unconverged SM-estimate
```


Now lets plot the data to visualize the details

```r
gapminder_mod%>%
    group_by(country) %>%
      ggplot(aes(year,lifeExp, group=country)) + 
      geom_point(color="blue") +
      geom_smooth(method = "rlm", se=FALSE,color="red", size=.25)+
      facet_wrap(~continent)
```

```
## Warning in rlm.default(x, y, weights, method = method, wt.method =
## wt.method, : 'rlm' failed to converge in 20 steps

## Warning in rlm.default(x, y, weights, method = method, wt.method =
## wt.method, : 'rlm' failed to converge in 20 steps

## Warning in rlm.default(x, y, weights, method = method, wt.method =
## wt.method, : 'rlm' failed to converge in 20 steps
```

![](hw06_files/figure-html/unnamed-chunk-9-1.png)<!-- -->
NOw we can analyze each continent to identify the continents that fit better and worse.

###Task 5 Working with the list

####Trump Android Tweets

Lets load the data from trump's official account

```r
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
#load("trump_tweets_df.rda")
glimpse(trump_tweets_df)
```

```
## Observations: 1,512
## Variables: 16
## $ text          <chr> "My economic policy speech will be carried live ...
## $ favorited     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,...
## $ favoriteCount <dbl> 9214, 6981, 15724, 19837, 34051, 29831, 19223, 1...
## $ replyToSN     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ created       <dttm> 2016-08-08 15:20:44, 2016-08-08 13:28:20, 2016-...
## $ truncated     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,...
## $ replyToSID    <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ id            <chr> "762669882571980801", "762641595439190016", "762...
## $ replyToUID    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ statusSource  <chr> "<a href=\"http://twitter.com/download/android\"...
## $ screenName    <chr> "realDonaldTrump", "realDonaldTrump", "realDonal...
## $ retweetCount  <dbl> 3107, 2390, 6691, 6402, 11717, 9892, 5784, 7930,...
## $ isRetweet     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,...
## $ retweeted     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,...
## $ longitude     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ latitude      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
```

```r
#Take just the text and store it in tweets
tweets <- trump_tweets_df$text
tweets %>% head() %>% strtrim(70) #trim the length to be 70
```

```
## [1] "My economic policy speech will be carried live at 12:15 P.M. Enjoy!"   
## [2] "Join me in Fayetteville, North Carolina tomorrow evening at 6pm. Ticke"
## [3] "#ICYMI: \"Will Media Apologize to Trump?\" https://t.co/ia7rKBmioA"    
## [4] "Michael Morell, the lightweight former Acting Director of C.I.A., and "
## [5] "The media is going crazy. They totally distort so many things on purpo"
## [6] "I see where Mayor Stephanie Rawlings-Blake of Baltimore is pushing Cro"
```

Create a regular expression of words that were commonly found in trump's tweets.

```r
regex <- "badly|crazy|weak|spent|strong|dumb|joke|guns|funny|dead"
```

Lets select preselect the rows that scale down the complexity of the problem.

```r
tweets <- tweets[c(1, 2, 5, 6, 198, 347, 919)]
tweets %>% strtrim(70)
```

```
## [1] "My economic policy speech will be carried live at 12:15 P.M. Enjoy!"   
## [2] "Join me in Fayetteville, North Carolina tomorrow evening at 6pm. Ticke"
## [3] "The media is going crazy. They totally distort so many things on purpo"
## [4] "I see where Mayor Stephanie Rawlings-Blake of Baltimore is pushing Cro"
## [5] "Bernie Sanders started off strong, but with the selection of Kaine for"
## [6] "Crooked Hillary Clinton is unfit to serve as President of the U.S. Her"
## [7] "The Cruz-Kasich pact is under great strain. This joke of a deal is fal"
```

Tweets with 0, 1, 2, and 3 occurences of Trump Android words were preselected .

####gregexpr()

Use the base function gregexpr() to locate all the Trump Android words inside the tweets.

```r
matches <- gregexpr(regex, tweets)
#str(matches)
```

Lets take a look at one element of matches

```r
matches[[7]]
```

```
## [1]  50 112 123
## attr(,"match.length")
## [1] 4 4 4
## attr(,"useBytes")
## [1] TRUE
```

Matches is A list. 
1. One element per element of tweets.
2. Each element is an integer vector.
   - It's -1 if no matches found.
   - Holds the position(s) of the first character of each match, otherwise.
3. Each element has two attributes. Consider match.length. Let us not speak of the other one.
   - It's -1 if no matches found.
   - Holds the length(s) of each match, otherwise.
We can clearly extract the matched words with this information. But its not easy from this information.

Lets inspect matches to understand its nature


```r
lengths(matches)                      # just happens to exist for length
```

```
## [1] 1 1 1 1 2 2 3
```

```r
sapply(matches, length)               # NSFP = not safe for programming
```

```
## [1] 1 1 1 1 2 2 3
```

```r
vapply(matches, length, integer(1))   # preferred base approach
```

```
## [1] 1 1 1 1 2 2 3
```

```r
map_int(matches, length)
```

```
## [1] 1 1 1 1 2 2 3
```

**Get the list of the match lengths**

This is how it is got for the last element of matches

```r
m <- matches[[7]]
attr(m, which = "match.length")
```

```
## [1] 4 4 4
```

For entire matches, it can be done in few ways

1. Pre-defined custom function. ( Most verbose.)

```r
ml <- function(x) attr(x, which = "match.length")
map(matches, ml)
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 5
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 6 4
## 
## [[6]]
## [1] 4 6
## 
## [[7]]
## [1] 4 4 4
```

2. Anonymous function. ( Very compact.)

```r
map(matches, function(x) attr(x, which = "match.length"))
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 5
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 6 4
## 
## [[6]]
## [1] 4 6
## 
## [[7]]
## [1] 4 4 4
```

3. Pre-existing function, additional arguments passed via ....

```r
(match_length <- map(matches, attr, which = "match.length"))
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 5
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 6 4
## 
## [[6]]
## [1] 4 6
## 
## [[7]]
## [1] 4 4 4
```

**Count the number of Trump Android words in each tweet.**

Code that works for extreme examples 0 matches and 3 matches:

```r
m <- matches[[1]]
sum(m > 0)
```

```
## [1] 0
```

```r
m <- matches[[7]]
sum(m > 0)
```

```
## [1] 3
```

Only two of the above approaches work here 


```r
f <- function(x) sum(x > 0)
map(matches, f)
```

```
## [[1]]
## [1] 0
## 
## [[2]]
## [1] 0
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1
## 
## [[5]]
## [1] 2
## 
## [[6]]
## [1] 2
## 
## [[7]]
## [1] 3
```

```r
map(matches, ~ sum(.x > 0))
```

```
## [[1]]
## [1] 0
## 
## [[2]]
## [1] 0
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1
## 
## [[5]]
## [1] 2
## 
## [[6]]
## [1] 2
## 
## [[7]]
## [1] 3
```

Simpler version is to use map_int which returns an integer vector, with length equal to the number of tweets.

```r
map_int(matches, ~ sum(.x > 0))
```

```
## [1] 0 0 1 1 2 2 3
```

To confirm lets check it is indeed, different from just taking the lengths of the elements of matches:

```r
tibble(
  naive_length = lengths(matches),
  n_words = map_int(matches, ~ sum(.x > 0))
)
```

```
## # A tibble: 7 x 2
##   naive_length n_words
##          <int>   <int>
## 1            1       0
## 2            1       0
## 3            1       1
## 4            1       1
## 5            2       2
## 6            2       2
## 7            3       3
```

####Strip the attributes from matches
Lets remove the attributes from matches to create matches_first


```r
(match_first <- map(matches, as.vector))
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 20
## 
## [[4]]
## [1] 134
## 
## [[5]]
## [1] 28 95
## 
## [[6]]
## [1]  87 114
## 
## [[7]]
## [1]  50 112 123
```

####Assess progress in a small example
Lets extract trump words from single tweet. We will take tweets #1 and #7 as they represent extreme cases where matches are 0 and 3.

The relevant R objects:

```r
tweets %>% strtrim(70)
```

```
## [1] "My economic policy speech will be carried live at 12:15 P.M. Enjoy!"   
## [2] "Join me in Fayetteville, North Carolina tomorrow evening at 6pm. Ticke"
## [3] "The media is going crazy. They totally distort so many things on purpo"
## [4] "I see where Mayor Stephanie Rawlings-Blake of Baltimore is pushing Cro"
## [5] "Bernie Sanders started off strong, but with the selection of Kaine for"
## [6] "Crooked Hillary Clinton is unfit to serve as President of the U.S. Her"
## [7] "The Cruz-Kasich pact is under great strain. This joke of a deal is fal"
```

```r
match_first
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 20
## 
## [[4]]
## [1] 134
## 
## [[5]]
## [1] 28 95
## 
## [[6]]
## [1]  87 114
## 
## [[7]]
## [1]  50 112 123
```

```r
match_length
```

```
## [[1]]
## [1] -1
## 
## [[2]]
## [1] -1
## 
## [[3]]
## [1] 5
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 6 4
## 
## [[6]]
## [1] 4 6
## 
## [[7]]
## [1] 4 4 4
```

Lets first work with tweet #7, the one with 3 matched Trump words.

```r
(tweet <- tweets[7])
```

```
## [1] "The Cruz-Kasich pact is under great strain. This joke of a deal is falling apart, not being honored and almost dead. Very dumb!"
```

```r
(t_first <- match_first[[7]])      #starting of the matched words
```

```
## [1]  50 112 123
```

```r
(t_length <- match_length[[7]])    #length of the matched words
```

```
## [1] 4 4 4
```

```r
(t_last <- t_first + t_length - 1) #ending of the matched words
```

```
## [1]  53 115 126
```

```r
substring(tweet, t_first, t_last) #get the substrings to get the matched words
```

```
## [1] "joke" "dead" "dumb"
```

Use this code for tweet #1 with 0 trump words

```r
(tweet <- tweets[1])
```

```
## [1] "My economic policy speech will be carried live at 12:15 P.M. Enjoy!"
```

```r
(t_first <- match_first[[1]])
```

```
## [1] -1
```

```r
(t_length <- match_length[[1]])
```

```
## [1] -1
```

```r
(t_last <- t_first + t_length - 1)
```

```
## [1] -3
```

```r
substring(tweet, t_first, t_last)
```

```
## [1] ""
```

It works correctly for both the extremes. 

####Store where Trump words end

Lets get where the matches end for all the tweets. We need to use map2 because we have to map over 2 lists in parallel namely, match_first and match_length .


```r
(match_last <- map2(match_first, match_length, ~ .x + .y - 1)) 
```

```
## [[1]]
## [1] -3
## 
## [[2]]
## [1] -3
## 
## [[3]]
## [1] 24
## 
## [[4]]
## [1] 137
## 
## [[5]]
## [1] 33 98
## 
## [[6]]
## [1]  90 119
## 
## [[7]]
## [1]  53 115 126
```

####Extract the trump words

Now lets extract the trump words. Here we need to map over three lists simulataneously
matches,matches_first and matches_last. So we use pmap to perform this.

```r
pmap(list(text = tweets, first = match_first, last = match_last), substring)
```

```
## [[1]]
## [1] ""
## 
## [[2]]
## [1] ""
## 
## [[3]]
## [1] "crazy"
## 
## [[4]]
## [1] "joke"
## 
## [[5]]
## [1] "strong" "weak"  
## 
## [[6]]
## [1] "weak"   "strong"
## 
## [[7]]
## [1] "joke" "dead" "dumb"
```

####March through the rows in a data frame

Lets use a dataframe as input to pmap to get our desired result.

```r
mdf <- tibble(
  text = tweets,
  first = match_first,
  last = match_last
)
pmap(mdf, substring)
```

```
## [[1]]
## [1] ""
## 
## [[2]]
## [1] ""
## 
## [[3]]
## [1] "crazy"
## 
## [[4]]
## [1] "joke"
## 
## [[5]]
## [1] "strong" "weak"  
## 
## [[6]]
## [1] "weak"   "strong"
## 
## [[7]]
## [1] "joke" "dead" "dumb"
```

Now lets see if we can reproduce everything using a  data frame approach.

```r
tibble(text = tweets,
      first = gregexpr(regex, tweets)) %>% 
      mutate(match_length = map(first, ~ attr(.x, which = "match.length")),
      last = map2(first, match_length, ~ .x + .y - 1))%>%
      dplyr::select(-match_length)%>% 
      pmap(substring)
```

```
## [[1]]
## [1] ""
## 
## [[2]]
## [1] ""
## 
## [[3]]
## [1] "crazy"
## 
## [[4]]
## [1] "joke"
## 
## [[5]]
## [1] "strong" "weak"  
## 
## [[6]]
## [1] "weak"   "strong"
## 
## [[7]]
## [1] "joke" "dead" "dumb"
```

We can directly solve this problem by post-processing the output of gregexpr() with regmatches()

```r
regmatches(tweets, gregexpr(regex, tweets))
```

```
## [[1]]
## character(0)
## 
## [[2]]
## character(0)
## 
## [[3]]
## [1] "crazy"
## 
## [[4]]
## [1] "joke"
## 
## [[5]]
## [1] "strong" "weak"  
## 
## [[6]]
## [1] "weak"   "strong"
## 
## [[7]]
## [1] "joke" "dead" "dumb"
```

We can check the base code of regmatches and find that it is similar to the way we have done this problem as it uses lot of calls to map(), attr() and substr() etc. But it has more error checking and consideration for encoding and locale. 

