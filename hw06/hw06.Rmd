---
title: "hw06.Rmd"
output: 
      html_document:
        keep_md: yes
---

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(ggmap))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(leaflet))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(singer))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(robustbase))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(MASS))
options(knitr.table.format = "markdown")
```

Lets update the theme of ggplot to make title of all plots centered.
```{r}
theme_update(plot.title = element_text(hjust = 0.5))
```


### Task 2 
### Writing functions

Lets define the functions for simple linear regression **le_lin_fit()** and robust regression such as **le_rlm_fit()** and **le_lmrob_fit()**. Lets plot the fitted linear model to check the kind of plots generated for fitted regression models.
```{r}
le_lin_fit <- function(data,offset = 1952){
the_fit <- lm(gdpPercap ~ I(year - offset), data)
  output <- data.frame(t(coef(the_fit))) %>%
  setNames(c("intercept", "slope"))
  plot(the_fit)             #plot the fitted model
  output
}

le_rlm_fit <- function(data,offset = 1952){
  fit <- rlm(gdpPercap ~ I(year - offset), data,method="M")
  #fitted(fit) # predicted values
  #residuals(fit) # residuals
  vcov(fit) # covariance matrix for model parameters 
  influence(fit) # regression diagnostics
}

le_lmrob_fit <- function(data,offset = 1952){
fit <- lmrob(gdpPercap ~ I(year - offset), data)
  #fitted(fit) # predicted values
  #residuals(fit) # residuals
  #vcov(fit) # covariance matrix for model parameters 
  #influence(fit) # regression diagnostics
}
```

Lets check the function with some data for India.

```{r}
j_dat<-subset(gapminder,country=="India")
```

Lets get the cofficients of simple regression function using inbuilt function le_lin_fit()
```{r}
le_lin_fit(j_dat)
```

* Upper left plot shows the residual errors plotted versus their fitted values. The residuals are randomly distributed around the horizontal line without any trends.
* Lower left plot is a standard Q-Q plot, which suggests that the residual errors are normally distributed. 
* Scale-location plot(upper right) shows the square root of the standardized residuals as a function of the fitted values with  no obvious trend in this plot.
* Finally, the plot in the lower right shows each points leverage, which is a measure of its
importance in determining the regression result. 
* Superimposed on the final plot are contour lines for the Cookâ€™s distance, which is another measure of the importance of each observation to the regression. 
  + Smaller distances means that removing the observation has little affect on the regression results. 
  + Distances larger than 1 are suspicious and suggest the presence of a possible outlier or a poor model. 

Same thing cane be done with other two functions. Note that these two functions dont produce plot but if needed it can be obtained.
```{r}
le_rlm_fit(j_dat)
le_lmrob_fit(j_dat)
```

Note that we get lot of parameters such as covariance matrix for model parameters (cov() functino) and regression diagnostics (influence() function). We can also get residual values and predicted values using residuals() and fitted() functions respectively.

Let plot this data with the trendline. 
Note: lmrob method is not available in geom_smooth() package.
```{r}
ggplot(j_dat, aes(x = year, y = gdpPercap))+ 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)

ggplot(j_dat, aes(x = year, y = gdpPercap))+ 
  geom_point() + 
  geom_smooth(method = "rlm", se = FALSE)
```

#### References
+ [Using R for Linear Regression](http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf)
+ [Stat Notes](http://stat545.com/block012_function-regress-lifeexp-on-year.html)
+ [Multiple linear regression - Quick R](https://www.statmethods.net/stats/regression.html)
+ [Simple Linear Regression - R Bloggers](https://www.r-bloggers.com/simple-linear-regression-2/)


### Task 4

### Working with Singer data

Lets get the singer data first.
```{r}
singer_data<- singer::singer_locations
```

Now lets define the possibly function to avoid the errors and produce NA when we get bad results from revgeocode().
```{r warning=FALSE}
poss_revgeocode <- 
  possibly(
    function(x,y) revgeocode(c(x, y), output = "more"), 
    otherwise = NA_real_,
    quiet = TRUE
    )
```

Now lets get the details for a sample 20 rows from singer data without na values in either latitude nor longitude and then get the address,locality, postal code details for each row from **revgeocode()** function. This is returned in a data frame which is then unnested to get in the actual table.

```{r message=FALSE}
new_singer_data <- singer_data %>%
                      filter(!is.na(longitude))%>%
                      filter(!is.na(longitude))%>%
                      as.tibble()%>%
                      sample_n(20) %>%
                      mutate(mapdata=map2(longitude,latitude,
                         ~ poss_revgeocode(.x,.y)))%>%
                      unnest()
```

#### Results against singer city

Lets view locality obtained from mapdata with city 
```{r}
new_singer_data %>%
  dplyr::select(locality,city)%>%
  head(20)%>%
  kable()
```

This shows that the city has to be adjusted just to give the actual name of the city ( the first part before comma).

```{r}
b <- new_singer_data%>%
                      separate(city, 
                               into = c("city1", "city2(code)"), 
                               sep = "\\,",
                               extra = "merge",
                               fill = "right")
```

Now lets compare the city1 with locality to obtain the number of rows matching
```{r}
b%>%
  filter(city1 == locality)%>%
  count()
```
This shows that 6 rows out of 20 are matching. More analysis can be done to improve this accuracy.

#### Plot
Now, lets checkout the library leaflet for plotting. We can plot the locality using addCircles() function.
```{r}
leaflet()  %>%   
  addTiles() %>%  
  addCircles(lng=b$longitude,lat=b$latitude,popup = b$locality)
```

#### Reference:
+ [Separate](http://tidyr.tidyverse.org/reference/separate.html)
+ [ggmap](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)
+ [Stack Overflow](https://stackoverflow.com/questions/22911642/applying-revgeocode-to-a-list-of-longitude-latitude-coordinates)

### Task 5 Working with the list

#### Trump Android Tweets

Lets load the data from trump's official account
```{r}
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
#load("trump_tweets_df.rda")
glimpse(trump_tweets_df)

#Take just the text and store it in tweets
tweets <- trump_tweets_df$text
tweets %>% head() %>% strtrim(70) #trim the length to be 70
```

Create a regular expression of words that were commonly found in trump's tweets.
```{r}
regex <- "badly|crazy|weak|spent|strong|dumb|joke|guns|funny|dead"
```

Lets select preselect the rows that scale down the complexity of the problem.
```{r}
tweets <- tweets[c(1, 2, 5, 6, 198, 347, 919)]
tweets %>% strtrim(70)
```

Tweets with 0, 1, 2, and 3 occurences of Trump Android words were preselected .

#### gregexpr()

Use the base function gregexpr() to locate all the Trump Android words inside the tweets.
```{r}
matches <- gregexpr(regex, tweets)
#str(matches)
```

Lets take a look at one element of matches
```{r}
matches[[7]]
```

Matches is A list. 
1. One element per element of tweets.
2. Each element is an integer vector.
   - It's -1 if no matches found.
   - Holds the position(s) of the first character of each match, otherwise.
3. Each element has two attributes. Consider match.length. Let us not speak of the other one.
   - It's -1 if no matches found.
   - Holds the length(s) of each match, otherwise.
We can clearly extract the matched words with this information. But its not easy from this information.

Lets inspect matches to understand its nature

```{r}
lengths(matches)                      # just happens to exist for length
sapply(matches, length)               # NSFP = not safe for programming
vapply(matches, length, integer(1))   # preferred base approach
map_int(matches, length)
```

**Get the list of the match lengths**

This is how it is got for the last element of matches
```{r}
m <- matches[[7]]
attr(m, which = "match.length")
```

For entire matches, it can be done in few ways

* Pre-defined custom function. ( Most verbose.)
```{r}
ml <- function(x) attr(x, which = "match.length")
map(matches, ml)
```

* Anonymous function. ( Very compact.)
```{r}
map(matches, function(x) attr(x, which = "match.length"))
```

* Pre-existing function, additional arguments passed via ....
```{r}
(match_length <- map(matches, attr, which = "match.length"))
```

**Count the number of Trump Android words in each tweet.**

Code that works for extreme examples 0 matches and 3 matches:
```{r}
m <- matches[[1]]
sum(m > 0)
m <- matches[[7]]
sum(m > 0)
```

**Only two of the above approaches work here**

```{r}
f <- function(x) sum(x > 0)
map(matches, f)

map(matches, ~ sum(.x > 0))
```

Simpler version is to use map_int which returns an integer vector, with length equal to the number of tweets.
```{r}
map_int(matches, ~ sum(.x > 0))
```

To confirm lets check it is indeed, different from just taking the lengths of the elements of matches:
```{r}
tibble(
  naive_length = lengths(matches),
  n_words = map_int(matches, ~ sum(.x > 0))
)
```

#### Strip the attributes from matches
Lets remove the attributes from matches to create matches_first

```{r}
(match_first <- map(matches, as.vector))
```

#### Assess progress in a small example
Lets extract trump words from single tweet. We will take tweets #1 and #7 as they represent extreme cases where matches are 0 and 3.

The relevant R objects:
```{r}
tweets %>% strtrim(70)
match_first
match_length
```

Lets first work with tweet #7, the one with 3 matched Trump words.
```{r}
(tweet <- tweets[7])
(t_first <- match_first[[7]])      #starting of the matched words
(t_length <- match_length[[7]])    #length of the matched words
(t_last <- t_first + t_length - 1) #ending of the matched words
substring(tweet, t_first, t_last) #get the substrings to get the matched words
```

Use this code for tweet #1 with 0 trump words
```{r}
(tweet <- tweets[1])
(t_first <- match_first[[1]])
(t_length <- match_length[[1]])
(t_last <- t_first + t_length - 1)
substring(tweet, t_first, t_last)
```

It works correctly for both the extremes. 

#### Store where Trump words end

Lets get where the matches end for all the tweets. We need to use map2 because we have to map over 2 lists in parallel namely, match_first and match_length .

```{r}
(match_last <- map2(match_first, match_length, ~ .x + .y - 1)) 
```

#### Extract the trump words

Now lets extract the trump words. Here we need to map over three lists simulataneously
matches,matches_first and matches_last. So we use pmap to perform this.
```{r}
pmap(list(text = tweets, first = match_first, last = match_last), substring)
```

#### March through the rows in a data frame

Lets use a dataframe as input to pmap to get our desired result.
```{r}
mdf <- tibble(
  text = tweets,
  first = match_first,
  last = match_last
)
pmap(mdf, substring)
```

Now lets see if we can reproduce everything using a  data frame approach.
```{r}
tibble(text = tweets,
      first = gregexpr(regex, tweets)) %>% 
      mutate(match_length = map(first, ~ attr(.x, which = "match.length")),
      last = map2(first, match_length, ~ .x + .y - 1))%>%
      dplyr::select(-match_length)%>% 
      pmap(substring)
```

We can directly solve this problem by post-processing the output of gregexpr() with regmatches()
```{r}
regmatches(tweets, gregexpr(regex, tweets))
```

We can check the base code of regmatches and find that it is similar to the way we have done this problem as it uses lot of calls to map(), attr() and substr() etc. But it has more error checking and consideration for encoding and locale. 

```{r}
regmatches
```

#### Reference:
+ [Purrr Tutorial](https://jennybc.github.io/purrr-tutorial/index.html)
